{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783b434-731f-438d-bf53-e09a653af90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'c:/MyDocs/integrated/') # adjust to your setup\n",
    "\n",
    "%run \"catalog_support.py\" \n",
    "\n",
    "today = datetime.datetime.today()\n",
    "edate = datetime.datetime(year=2024,month=3,day=5)\n",
    "Numdays = (today - edate).days\n",
    "\n",
    "showHeader('Raw Disclosures',line2=f'{Numdays} days of FracFocus changes',use_remote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01ba08-8fd4-4556-a835-28674be213ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "last_repo = datetime.datetime(year=2025,month=5,day=14)\n",
    "\n",
    "# edate = today - datetime.timedelta(days=Numdays)\n",
    "# print('earlist shown',edate)\n",
    "\n",
    "daily_status_folder = r\"G:\\My Drive\\webshare\\daily_status\"\n",
    "added_disc_fn = os.path.join(daily_status_folder,'added_disc_df.parquet')\n",
    "removed_disc_fn = os.path.join(daily_status_folder,'removed_disc_df.parquet')\n",
    "modified_disc_fn = os.path.join(daily_status_folder,'modified_disc_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56e618-e533-4c3f-b28c-3525036f27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import openFF.common.text_handlers as th\n",
    "# arc_dir = r\"D:\\openFF_archive\\diff_dicts\"\n",
    "#!\n",
    "# arc_dir = r\"C:\\MyDocs\\integrated\\openFF_archive\\diff_dicts\"\n",
    "arc_dir = r\"G:\\My Drive\\production\\openFF_archive\\diff_dicts\"\n",
    "\n",
    "diff_fns = os.listdir(arc_dir)\n",
    "download_dates = []\n",
    "added = []\n",
    "changed = []\n",
    "removed = []\n",
    "casing = set()\n",
    "operator = set()\n",
    "\n",
    "\n",
    "for fn in diff_fns:\n",
    "    tdate = datetime.datetime(int(fn[10:14]),int(fn[15:17]),int(fn[18:20]))\n",
    "    # print(tdate)\n",
    "    if tdate>= edate:\n",
    "        download_dates.append(fn[10:20])\n",
    "        with open(os.path.join(arc_dir,fn),'rb') as f:\n",
    "            diff_dic = pickle.load(f)\n",
    "        if len(diff_dic['removed_disc'])>0:\n",
    "            t = diff_dic['removed_disc'].copy()\n",
    "            # print(t.head())\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'removed'\n",
    "            removed.append(t)\n",
    "        if len(diff_dic['added_disc'])>0:\n",
    "            t = diff_dic['added_disc'].copy()\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'added'\n",
    "            added.append(t)\n",
    "        if len(diff_dic['changed_disc'])>0:\n",
    "            t = diff_dic['changed_disc'].copy()\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'modified'            \n",
    "            changed.append(t)\n",
    "        if len(diff_dic['casing'])>0:\n",
    "            for item in diff_dic['casing']:\n",
    "                # print(item)\n",
    "                if item[1]==None: ig = ''\n",
    "                else: ig = item[1].strip().lower()\n",
    "                tup = (item[0],ig)\n",
    "                casing.add(tup)\n",
    "        if len(diff_dic['OperatorName'])>0:\n",
    "            for item in diff_dic['OperatorName']:\n",
    "                operator.add(item)\n",
    "\n",
    "alllists = added + changed + removed\n",
    "wholeset = pd.concat(alllists,sort=True)\n",
    "# print(f'Len wholeset: {len(wholeset)}')\n",
    "# wholeset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d462b-e9a5-4ce5-a83b-ca174872603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_added = wholeset[wholeset.change_type=='added'].groupby('date_changed').size()\n",
    "added_sum = gb_added.resample(\"D\").sum()\n",
    "added_sum = added_sum+.001 # to distinguish between zeros and no data\n",
    "\n",
    "gb_removed = wholeset[wholeset.change_type=='removed'].groupby('date_changed').size()\n",
    "removed_sum = gb_removed.resample(\"D\").sum()\n",
    "removed_sum = removed_sum+0.001\n",
    "\n",
    "gb_changed = wholeset[wholeset.change_type=='modified'].groupby('date_changed').size()\n",
    "changed_sum = gb_changed.resample(\"D\").sum()\n",
    "changed_sum = changed_sum+0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382bc8d-d799-4ec9-ba0e-f774862d3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### set up the difflib outputs\n",
    "\n",
    "import difflib\n",
    "archive_folder = r\"G:\\My Drive\\production\\openFF_archive\\raw_dataframes\"  #drive\n",
    "# archive_folder = r\"C:\\MyDocs\\integrated\\openFF_archive\\raw_dataframes\" # production machine\n",
    "\n",
    "def get_raw_list():\n",
    "    alst = os.listdir(archive_folder)\n",
    "    dates = []\n",
    "    for f in alst:\n",
    "        dates.append(f[7:17])\n",
    "    return dates\n",
    "    \n",
    "def get_early_late_fn(change_date,datelst):\n",
    "    latefn = os.path.join(archive_folder,'raw_df_'+change_date+'.parquet')\n",
    "    idx = datelst.index(change_date)\n",
    "    earlyfn = os.path.join(archive_folder,'raw_df_'+datelst[idx-1]+'.parquet')\n",
    "    # print(earlyfn+'\\n'+latefn)\n",
    "    return earlyfn,latefn\n",
    "\n",
    "def get_comparison_dfs(apinum,eDId,lDId,earlyfn,latefn):\n",
    "    # Have to fetch by discID otherwise will pickup multiple disclosures\n",
    "    filt = [('DisclosureId','==',eDId)]\n",
    "    edf = pd.read_parquet(earlyfn,filters=filt)\n",
    "    filt = [('DisclosureId','==',lDId)]\n",
    "    ldf = pd.read_parquet(latefn,filters=filt)\n",
    "    return edf,ldf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30257dc-6909-40ab-8ed7-8d427e7bfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_local_diff_fn(api,change_date):\n",
    "    # return os.path.join(r\"C:\\Users\\Gary\\My Drive\\webshare\\daily_status\\diff_files\",api+'_'+change_date+'.html')\n",
    "    return os.path.join(r\"G:\\My Drive\\webshare\\daily_status\\diff_files\",api+'_'+change_date+'.html')\n",
    "\n",
    "def get_diff_url(api,change_date):\n",
    "    return \"https://storage.googleapis.com/open-ff-browser/difflib/\"+api+'_'+change_date+'.html'\n",
    "\n",
    "def make_logistics_summary(ddf):\n",
    "    # assumes single disclosure in ddf\n",
    "    # start with meta or logistics data, so all is available in any line\n",
    "    jsd = ddf.JobStartDate.iloc[0]\n",
    "    jed = ddf.JobEndDate.iloc[0]\n",
    "    state = ddf.StateName.iloc[0]\n",
    "    county = ddf.CountyName.iloc[0]\n",
    "    api = ddf.APINumber.iloc[0]\n",
    "    operator = ddf.OperatorName.iloc[0]\n",
    "    wellname = ddf.WellName.iloc[0]\n",
    "    lat = ddf.Latitude.iloc[0]\n",
    "    lon = ddf.Longitude.iloc[0]\n",
    "    datum = ddf.Projection.iloc[0]\n",
    "    fed = ddf.FederalWell.iloc[0]\n",
    "    indian = ddf.IndianWell.iloc[0]\n",
    "    tvd = ddf.TVD.iloc[0]\n",
    "    tbwv = ddf.TotalBaseWaterVolume.iloc[0]\n",
    "    tbnwv = ddf.TotalBaseNonWaterVolume.iloc[0]\n",
    "    did = ddf.DisclosureId.iloc[0]\n",
    "    ffv = ddf.FFVersion.iloc[0]\n",
    "\n",
    "    s=  '\\n**Logistics Section** \\n'\n",
    "    s+= f'APINumber: {api}\\n'\n",
    "    s+= f'Job Start Date: {jsd}\\n'\n",
    "    s+= f'Job End Date: {jed}\\n'\n",
    "    s+= f'State: {state}\\n'\n",
    "    s+= f'County: {county}\\n'\n",
    "    s+= f'Operator: {operator}\\n'\n",
    "    s+= f'Well Name: {wellname}\\n'\n",
    "    s+= f'Latitude: {lat}  Longitude: {lon}\\n'\n",
    "    s+= f'Projection: {datum}\\n'\n",
    "    s+= f'Federal: {fed}   Indian: {indian}\\n'\n",
    "    s+= f'Total Vertical Depth: {tvd}\\n'\n",
    "    s+= f'Carrier water volume: {tbwv}\\n'\n",
    "    s+= f'Vol of non-water carrier: {tbnwv}\\n'\n",
    "    s+= f'FF Version: {ffv}\\n'\n",
    "    s+= f'DisclosureId: {did}\\n'\n",
    "\n",
    "    return s\n",
    "\n",
    "def make_products_summary(ddf):\n",
    "    ###  TradeName products\n",
    "    ddf.CASNumber = ddf.CASNumber.fillna('--empty--')\n",
    "    ddf = ddf.sort_values(['TradeName','CASNumber'])\n",
    "    gb = ddf.groupby('TradeName')['CASNumber'].apply(set).reset_index()\n",
    "    s= '\\n**Products List Section with components**\\n'\n",
    "    for i,row in gb.iterrows():\n",
    "        s+= f'Name: {row.TradeName}\\n'\n",
    "        lst = list(row.CASNumber)\n",
    "        lst.sort()\n",
    "        for cas in lst:\n",
    "            s+= f'      - {cas}\\n'\n",
    "    return s\n",
    "    # tn = ddf.TradeName.unique().tolist()\n",
    "    # tn.sort()\n",
    "    # for i in tn:\n",
    "    #     s+= f'Name: {i}\\n'\n",
    "\n",
    "def make_record_count_summary(ddf):\n",
    "    ###  Number of records\n",
    "    cn = ddf.CASNumber.notna()&ddf.PercentHFJob.notna()\n",
    "    s= '\\n**Number of records Section**\\n'\n",
    "    s+= f'Total num records: {len(ddf)}\\n'\n",
    "    s+= f'Record with chem info: {len(ddf[cn])}\\n'\n",
    "    return s\n",
    "\n",
    "def make_num_duplicates(ddf):\n",
    "    cn = ddf.CASNumber.notna()&ddf.IngredientName.notna()&ddf.PercentHighAdditive.notna()&ddf.PercentHFJob.notna()\n",
    "    cdup = ddf[['CASNumber','IngredientName','PercentHighAdditive','PercentHFJob','MassIngredient']].duplicated()\n",
    "    s = '\\n**Number of duplicate records**\\n'\n",
    "    t = ddf[cn&cdup].copy()\n",
    "    s += f'Number of duplicate records: {len(t)}\\n'\n",
    "    # for i,row in t.iterrows():\n",
    "    #     s += f'duplicated record: {row.CASNumber}, {row.IngredientName}, {row.PercentHFJob}\\n'\n",
    "    return s\n",
    "\n",
    "def make_chem_list(ddf):\n",
    "    #  List of UNIQUE CAS/ING\n",
    "    s = '\\n**Unique CASNumber/IngredientName pairs**\\n'\n",
    "    t = ddf.groupby(['CASNumber','IngredientName'],as_index=False).size().rename({'size':'num_recs'},axis=1)\n",
    "    t = t.sort_values(['CASNumber','IngredientName'])\n",
    "    for i,row in t.iterrows():\n",
    "        s += f'CAS: {row.CASNumber}, Name: {row.IngredientName}, How many: {row.num_recs}\\n'\n",
    "    return s\n",
    "\n",
    "def make_chem_data_list(ddf):\n",
    "    # list of main components of a data record - only those with CASNumber or IngredientName\n",
    "    s = '\\n**record data**\\n\\n'\n",
    "    sortby_order = ['TradeName','Purpose','Supplier','PercentHFJob','CASNumber','IngredientName']\n",
    "    # t = ddf.groupby(['CASNumber','IngredientName'],as_index=False).size().rename({'size':'num_recs'},axis=1)\n",
    "    t = ddf.sort_values(sortby_order)\n",
    "    for i,row in t.iterrows():\n",
    "        s += f'{row.TradeName};{row.Purpose};{row.Supplier}\\n\\t{row.CASNumber}\\n\\t{row.IngredientName}\\n\\t{row.PercentHighAdditive};{row.PercentHFJob};{row.MassIngredient}\\n'\n",
    "    return s\n",
    "    \n",
    "# def make_diff_str(str1,str2,section='logistics'):\n",
    "#     diff = difflib.context_diff(str1, str2,n=20,fromfile='', tofile='')\n",
    "#     s = f'\\n\\n<<{section}>>\\n'\n",
    "#     for line in diff:\n",
    "#         s+= line\n",
    "#     # print(f'*****   DIFF FILE FOLLOWS *****\\n{s}')\n",
    "#     return s\n",
    "\n",
    "def make_html_diff(str1,str2):\n",
    "    HD = difflib.HtmlDiff(wrapcolumn=80)\n",
    "    return HD.make_file(str1, str2,fromdesc='original', todesc='modified')\n",
    "\n",
    "def get_diff_size(str1,str2):\n",
    "    diff = difflib.context_diff(str1, str2,fromfile='original', tofile='modified')\n",
    "    s = ''\n",
    "    for line in diff:\n",
    "        s+= line\n",
    "    return len(s)\n",
    "\n",
    "# def comparison_alerts(edf,ldf):\n",
    "    \n",
    "    \n",
    "def PDF_like_compare(apinumber,eDId,lDId,change_date,datelst,outhtml):\n",
    "    print(f'working on {apinumber} {change_date}')\n",
    "    earlyfn,latefn = get_early_late_fn(change_date=change_date,datelst=datelst)\n",
    "    edf,ldf = get_comparison_dfs(apinumber,eDId,lDId,earlyfn,latefn)\n",
    "    try:\n",
    "        str1 = f'Archive file :\\n\\t{earlyfn}\\n'\n",
    "        str1 += make_logistics_summary(edf)\n",
    "        str1 += make_products_summary(edf)\n",
    "        str1 += make_record_count_summary(edf)\n",
    "        str1 += make_num_duplicates(edf)\n",
    "        str1 += make_chem_list(edf)\n",
    "        str1 += make_chem_data_list(edf)\n",
    "        lst1 = str1.split('\\n')\n",
    "        str2 = f'Archive file :\\n\\t{latefn}\\n'\n",
    "        str2 += make_logistics_summary(ldf)\n",
    "        str2 += make_products_summary(ldf)\n",
    "        str2 += make_record_count_summary(ldf)\n",
    "        str2 += make_num_duplicates(ldf)\n",
    "        str2 += make_chem_list(ldf)\n",
    "        str2 +=make_chem_data_list(ldf)\n",
    "        lst2 = str2.split('\\n')\n",
    "        # print(str1)\n",
    "        out  = make_html_diff(lst1,lst2)\n",
    "        with open(outhtml,'w') as f:\n",
    "            f.write(out)\n",
    "        return(get_diff_size(lst1,lst2))\n",
    "    except:\n",
    "        print('exception in PDF_like_comparison')\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cd6de-13bc-46ff-8a28-dc723e3fb4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2776d593-c3b3-4ae5-a4d3-7673d957be24",
   "metadata": {},
   "source": [
    "## Summary of ECMC disclosure list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a93e10-1de7-43e6-92d9-6b740ea4e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current ECMC disclosures and add to table\n",
    "url = \"https://ecmc.state.co.us/depot/Stats/ChemicalDisclosures/Index/Locations\"\n",
    "tables = pd.read_html(url)\n",
    "t = tables[0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7211686-cfe4-45ab-9e55-b6b6b922f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is gemini code to archive changed location pages at ECMC and create difflib logs of changes.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import sys\n",
    "import difflib\n",
    "\n",
    "# --- Configuration ---\n",
    "INDEX_URL = \"https://ecmc.state.co.us/depot/Stats/ChemicalDisclosures/Index/Locations\"\n",
    "ARCHIVE_DIR = Path(r\"G:\\My Drive\\production\\openFF_archive\\ecmc_loc_data\")\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_page_content(url, session):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding or 'utf-8'\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_location_number(url):\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        parts = path.strip('/').split('/')\n",
    "        if len(parts) > 0 and parts[-1].isdigit():\n",
    "             if len(parts) > 1 and parts[-2].lower() == 'location':\n",
    "                 return parts[-1]\n",
    "             else:\n",
    "                 logging.warning(f\"URL {url} structure might differ, but found trailing number: {parts[-1]}\")\n",
    "                 return parts[-1]\n",
    "        logging.warning(f\"Could not extract location number from URL: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing URL {url} for location number: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_most_recent_archive(location_num, archive_dir):\n",
    "    \"\"\"\n",
    "    Finds the most recent ARCHIVE file (excluding difflib files)\n",
    "    for a given location number.\n",
    "    \"\"\"\n",
    "    prefix = f\"Loc{location_num}_\"\n",
    "    try:\n",
    "        all_matching_files = list(archive_dir.glob(f\"{prefix}*.html\"))\n",
    "        archive_files = [\n",
    "            f for f in all_matching_files\n",
    "            if not f.name.endswith(\"_difflib.html\")\n",
    "        ]\n",
    "        if not archive_files:\n",
    "            return None\n",
    "        archive_files.sort()\n",
    "        return archive_files[-1]\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Error accessing archive directory {archive_dir} while searching for files: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_relevant_content(html_content):\n",
    "    \"\"\"\n",
    "    Parses HTML and extracts text content from specific sections\n",
    "    relevant for change detection. Normalizes whitespace.\n",
    "    Returns a single string of relevant text, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return None\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        relevant_texts = []\n",
    "        data_container_divs = soup.find_all('div', class_='mx-auto p-2 w-md-75 w-100', limit=3)\n",
    "\n",
    "        if len(data_container_divs) >= 1:\n",
    "            text1 = ' '.join(data_container_divs[0].get_text(separator=' ', strip=True).split())\n",
    "            relevant_texts.append(text1)\n",
    "        if len(data_container_divs) >= 3:\n",
    "             text3 = ' '.join(data_container_divs[2].get_text(separator=' ', strip=True).split())\n",
    "             relevant_texts.append(text3)\n",
    "        else:\n",
    "            logging.debug(\"Main container structure (3 divs) not found/matched. Falling back to direct table search.\")\n",
    "            tables = soup.find_all('table', class_='b-table table table-striped table-sm')\n",
    "            if tables:\n",
    "                 for i, table in enumerate(tables):\n",
    "                     table_text = ' '.join(table.get_text(separator=' ', strip=True).split())\n",
    "                     if table_text not in relevant_texts:\n",
    "                          relevant_texts.append(table_text)\n",
    "                 logging.debug(f\"Found {len(tables)} table(s) via fallback.\")\n",
    "            else:\n",
    "                # Only warn if the primary method also failed\n",
    "                if len(data_container_divs) < 1:\n",
    "                     logging.warning(\"Could not find relevant data containers or tables.\")\n",
    "\n",
    "        if not relevant_texts:\n",
    "            logging.error(\"Failed to extract any relevant content chunks.\")\n",
    "            return None\n",
    "\n",
    "        return \"\\n---\\n\".join(relevant_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing HTML with BeautifulSoup: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "def ECMC_archive_main():\n",
    "    # --- Logging Setup ---\n",
    "    log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO) # Set level (INFO, DEBUG, etc.)\n",
    "\n",
    "    try:\n",
    "        ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Archive directory checked/created: {ARCHIVE_DIR}\")\n",
    "    except OSError as e:\n",
    "        print(f\"CRITICAL: Could not create or access archive directory {ARCHIVE_DIR}: {e}\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    log_file_path = ARCHIVE_DIR / \"todays-log.txt\"\n",
    "    try:\n",
    "        file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')\n",
    "        file_handler.setFormatter(log_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        print(f\"Logging to file: {log_file_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"ERROR: Could not create or write to log file {log_file_path}: {e}\", file=sys.stderr)\n",
    "        print(\"WARNING: Proceeding without file logging.\", file=sys.stderr)\n",
    "\n",
    "    # --- Start of Process ---\n",
    "    logging.info(\"Starting archive process...\")\n",
    "    today_date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_files_saved = 0\n",
    "    updated_files_saved = 0\n",
    "    diff_files_saved = 0\n",
    "    links_processed = 0\n",
    "    links_failed = 0\n",
    "    links_found_count = 0\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update({'User-Agent': USER_AGENT})\n",
    "\n",
    "        logging.info(f\"Fetching index page: {INDEX_URL}\")\n",
    "        index_content = get_page_content(INDEX_URL, session)\n",
    "        if not index_content:\n",
    "            logging.critical(f\"Failed to fetch index page. Exiting.\")\n",
    "            print(\"CRITICAL: Failed to fetch index page. Exiting.\", file=sys.stderr)\n",
    "            return\n",
    "        soup = BeautifulSoup(index_content, 'html.parser')\n",
    "        location_links = soup.find_all('a', href=lambda href: href and '/Location/' in href)\n",
    "        links_found_count = len(location_links)\n",
    "        logging.info(f\"Found {links_found_count} potential location links.\")\n",
    "        LINK_BASE_URL = \"https://ecmc.state.co.us/depot/\"\n",
    "\n",
    "        for link in location_links:\n",
    "            links_processed += 1\n",
    "            location_url = \"\"\n",
    "            try:\n",
    "                relative_url = link.get('href')\n",
    "                if not relative_url:\n",
    "                    logging.warning(f\"Link tag found without href attribute. Skipping.\")\n",
    "                    links_failed += 1\n",
    "                    continue\n",
    "\n",
    "                location_url = urljoin(LINK_BASE_URL, relative_url)\n",
    "                location_num = extract_location_number(location_url)\n",
    "                if not location_num:\n",
    "                    links_failed += 1\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Processing Location: {location_num} ({location_url})\")\n",
    "\n",
    "                target_filename = ARCHIVE_DIR / f\"Loc{location_num}_{today_date_str}.html\"\n",
    "                most_recent_file = get_most_recent_archive(location_num, ARCHIVE_DIR)\n",
    "\n",
    "                current_content_full = get_page_content(location_url, session)\n",
    "                if current_content_full is None:\n",
    "                    logging.warning(f\"Failed to fetch content for location {location_num}. Skipping.\")\n",
    "                    links_failed += 1\n",
    "                    continue\n",
    "\n",
    "                current_relevant_content = extract_relevant_content(current_content_full)\n",
    "                if current_relevant_content is None:\n",
    "                    logging.warning(f\"Failed to extract relevant content from current page for {location_num}. Skipping comparison.\")\n",
    "                    links_failed += 1\n",
    "                    continue\n",
    "\n",
    "                save_needed = False\n",
    "                is_new = False\n",
    "                archived_content_full = None\n",
    "                archived_relevant_content = None\n",
    "\n",
    "                if most_recent_file:\n",
    "                    logging.info(f\"Checking against most recent archive: {most_recent_file.name}\")\n",
    "                    try:\n",
    "                        archived_content_full = most_recent_file.read_text(encoding='utf-8')\n",
    "                        archived_relevant_content = extract_relevant_content(archived_content_full)\n",
    "\n",
    "                        if archived_relevant_content is None:\n",
    "                            logging.warning(f\"Failed to extract relevant content from archived file {most_recent_file.name}. Treating as changed.\")\n",
    "                            save_needed = True\n",
    "                        elif current_relevant_content != archived_relevant_content:\n",
    "                            logging.info(f\"RELEVANT content changed for Location {location_num}. Archiving update.\")\n",
    "                            save_needed = True\n",
    "                        else:\n",
    "                            logging.info(f\"Relevant content unchanged for Location {location_num}. No archive needed today.\")\n",
    "\n",
    "                    except IOError as e:\n",
    "                        logging.error(f\"Error reading archive file {most_recent_file} for comparison: {e}\")\n",
    "                        logging.info(f\"Marking Location {location_num} for update due to error reading previous archive.\")\n",
    "                        save_needed = True\n",
    "\n",
    "                    # --- Generate Diff using RELEVANT content if change detected ---\n",
    "                    # Check we have relevant content from archive AND save is needed based on relevant comparison\n",
    "                    if save_needed and archived_relevant_content is not None:\n",
    "                         try:\n",
    "                             logging.info(f\"Generating diff file from RELEVANT content for Location {location_num}...\")\n",
    "                             diff_filename = ARCHIVE_DIR / f\"{target_filename.stem}_difflib.html\"\n",
    "\n",
    "                             # *** Use RELEVANT content strings, split into lines ***\n",
    "                             old_lines = archived_relevant_content.splitlines()\n",
    "                             new_lines = current_relevant_content.splitlines()\n",
    "                             # *****************************************************\n",
    "\n",
    "                             differ = difflib.HtmlDiff(tabsize=4, wrapcolumn=80)\n",
    "                             # Generate the HTML diff using the relevant lines\n",
    "                             diff_html = differ.make_file(\n",
    "                                 old_lines, new_lines,\n",
    "                                 # Update labels to reflect content source\n",
    "                                 fromdesc=f\"Archived Relevant Section: {most_recent_file.name}\",\n",
    "                                 todesc=f\"Current Relevant Section: {location_num} ({current_timestamp})\",\n",
    "                                 context=True, numlines=5 # Show context lines\n",
    "                             )\n",
    "                             # Save the generated HTML diff\n",
    "                             diff_filename.write_text(diff_html, encoding='utf-8')\n",
    "                             logging.info(f\"Successfully saved diff of relevant sections: {diff_filename.name}\")\n",
    "                             diff_files_saved += 1\n",
    "                         except Exception as e:\n",
    "                             # Log error if diff generation fails\n",
    "                             logging.error(f\"Could not generate or save diff file from relevant content for {target_filename.stem}: {e}\")\n",
    "                    # --- End Diff Generation ---\n",
    "\n",
    "                else: # No most_recent_file exists\n",
    "                    logging.info(f\"No previous archive found for Location {location_num}. Archiving.\")\n",
    "                    save_needed = True\n",
    "                    is_new = True\n",
    "\n",
    "                # Save the main FULL archive file if needed\n",
    "                if save_needed:\n",
    "                    try:\n",
    "                        target_filename.write_text(current_content_full, encoding='utf-8')\n",
    "                        logging.info(f\"Successfully saved FULL content: {target_filename.name}\")\n",
    "                        if is_new:\n",
    "                            new_files_saved += 1\n",
    "                        else:\n",
    "                            updated_files_saved += 1\n",
    "                    except IOError as e:\n",
    "                        logging.error(f\"Failed to write file {target_filename}: {e}\")\n",
    "                        links_failed += 1\n",
    "                    except Exception as e:\n",
    "                         logging.error(f\"Unexpected error writing file {target_filename}: {e}\")\n",
    "                         links_failed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred processing link {location_url or link.get('href', 'N/A')}: {e}\")\n",
    "                links_failed += 1\n",
    "\n",
    "    # --- Log Summary to File ---\n",
    "    logging.info(\"--- Archive Process Summary ---\")\n",
    "    logging.info(f\"Date: {today_date_str}\")\n",
    "    logging.info(f\"Total links found matching pattern: {links_found_count}\")\n",
    "    logging.info(f\"Links processed attempt: {links_processed}\")\n",
    "    logging.info(f\"New archives saved: {new_files_saved}\")\n",
    "    logging.info(f\"Updated archives saved: {updated_files_saved}\")\n",
    "    logging.info(f\"Diff files saved: {diff_files_saved}\")\n",
    "    logging.info(f\"Links skipped/failed: {links_failed}\")\n",
    "    logging.info(\"Archive process finished.\")\n",
    "\n",
    "    # --- Print Summary to Standard Output ---\n",
    "    print(\"\\n--- ECMC Archive Process Summary ---\")\n",
    "    # print(f\"Date: {today_date_str}\")\n",
    "    print(f\"Log file: {log_file_path}\")\n",
    "    print(f\"Total links found matching pattern: {links_found_count}\")\n",
    "    print(f\"Links processed attempt: {links_processed}\")\n",
    "    print(f\"New archives saved: {new_files_saved}\")\n",
    "    print(f\"Updated archives saved: {updated_files_saved}\")\n",
    "    print(f\"Diff files saved: {diff_files_saved}\")\n",
    "    print(f\"Links skipped/failed: {links_failed}\")\n",
    "    print(\"-------------------------------\\n\")\n",
    "\n",
    "\n",
    "ECMC_archive_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9700a-b021-4f7a-8fe3-21cad0e9b704",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Downloads from FracFocus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ef4ae-b390-4270-b074-22da5b7e376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset['job_end_date'] = wholeset.JobEndDate.str.split().str[0]\n",
    "wholeset['job_end_date'] = pd.to_datetime(wholeset.job_end_date,format=\"%m/%d/%Y\")\n",
    "wholeset['FF_disc'] = wholeset.apply(lambda x: th.getFFLink(x), axis=1)\n",
    "wholeset['disc_link'] = wholeset.apply(lambda x: th.getDisclosureLink(APINumber=x.APINumber,\n",
    "                                                                      disclosureid=x.discID,\n",
    "                                                                      text_to_show='Open-FF disclosure',\n",
    "                                                                      use_remote=True,\n",
    "                                                                      check_if_exists=False), axis=1)\n",
    "cond = (wholeset.date_changed>=last_repo)&(wholeset.change_type!='removed')\n",
    "wholeset.disc_link = np.where(cond,' ',wholeset.disc_link)\n",
    "wholeset['TBWV'] = wholeset.TotalBaseWaterVolume.astype('float64')\n",
    "wholeset['TBWV'] = wholeset.TBWV.map(lambda x: th.round_sig(x,5))\n",
    "wholeset['TBWV'] = wholeset.TBWV.fillna(' - ')\n",
    "# wholeset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8060d8-bf7e-4a55-a502-41f4fa1da722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of data set records: {len(wholeset)}')\n",
    "wholeset.change_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd7ba5-f204-4a4c-aed1-1c2a8a654687",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset['has more than one'] = np.where(wholeset.APINumber.duplicated(keep=False),'APINumber dupe','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4737e-af11-4f58-bc3e-127630e4b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curated disclosures, get detected flaws\n",
    "\n",
    "DiDs = wholeset.discID.unique().tolist()\n",
    "rec_iss = pd.read_parquet(os.path.join(hndl.curr_repo_dir,'record_issues.parquet'),columns=['r_flags','reckey'])\n",
    "reck = pd.read_parquet(os.path.join(hndl.curr_repo_pkl_dir,'chemrecs.parquet'),columns=['DisclosureId','reckey'])\n",
    "mg = pd.merge(rec_iss,reck,on='reckey',how='left')\n",
    "gb = mg[mg.DisclosureId.isin(DiDs)].groupby('DisclosureId',as_index=False)['r_flags'].apply(set)\n",
    "\n",
    "# fix situation where there are no DiDs from the record_issues df\n",
    "if len(gb)==0:\n",
    "    gb = pd.DataFrame(columns=mg.columns)\n",
    "\n",
    "\n",
    "def str_from_set(x):\n",
    "    s = ''\n",
    "    for item in x:\n",
    "        s += item +' '\n",
    "    return s\n",
    "\n",
    "gb['recstr'] = gb.r_flags.map(lambda x: str_from_set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9995-b518-47fc-b64b-e9184ef8e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_iss = pd.read_parquet(os.path.join(hndl.curr_repo_dir,'disclosure_issues.parquet'),columns=['d_flags','DisclosureId'])\n",
    "dis_iss = dis_iss[dis_iss.DisclosureId.isin(DiDs)]\n",
    "gb = gb.merge(dis_iss,on='DisclosureId',how='outer')\n",
    "gb = gb.fillna('')\n",
    "gb['issues'] = gb.d_flags +' '+gb.recstr\n",
    "wholeset = wholeset.merge(gb[['DisclosureId','issues']],left_on='discID',right_on='DisclosureId',how='left')\n",
    "wholeset.issues = wholeset.issues.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0f5d9-e25a-4300-bc97-75e500f58d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## watch list summary\n",
    "## See bottom of page for whole list\n",
    "# url = 'https://raw.githubusercontent.com/gwallison/FF_issues/master/watch_list.csv'\n",
    "# wdf = pd.read_csv(url,dtype = {'APINumber':'str'})\n",
    "url = 'https://raw.githubusercontent.com/gwallison/FF_issues/master/watch_list_master.parquet'\n",
    "wdf = pd.read_parquet(url)\n",
    "wdf = wdf.rename({'DisclosureId':'wl_DisclosureId'},axis=1)\n",
    "# wdf.date_entered = pd.to_datetime(wdf.date_entered,format='%m/%d/%y')\n",
    "# wdf.FF_report_date = pd.to_datetime(wdf.FF_report_date,format='%m/%d/%y')\n",
    "# wdf.Blog_date = pd.to_datetime(wdf.Blog_date,format='%m/%d/%y')\n",
    "# wdf.FF_updates = pd.to_datetime(wdf.FF_updates,format='%m/%d/%y')\n",
    "\n",
    "apis = wdf.APINumber.unique().tolist()\n",
    "\n",
    "watchlist_found = pd.merge(wdf,wholeset,on='APINumber',how='inner')\n",
    "# watchlist_found[['wl_name','change_type']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67e911-5989-4a29-9302-ae8b013d0bb6",
   "metadata": {},
   "source": [
    "## Pattern of new disclosure additions\n",
    "These disclosures are detected as new because their `DisclosureId` number hasn't been in the database before.  Note that it is possible that they are a new version of a previously published disclosure; sometimes operators change disclosures by removing the old one from FracFocus and creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d9ce-4d0f-4164-be4c-aee022002f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(download_dates)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import calplot\n",
    "calplot.calplot(added_sum,  cmap='Spectral_r', yearlabel_kws={'fontname':'sans-serif'});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79219256-de05-4fb5-9909-ecf97da76da5",
   "metadata": {},
   "source": [
    "- **Blue line** = Cumulative new disclosures added (my include replacements for removed disclosures)\n",
    "- **Orange line** = New disclosures with detected issues\n",
    "- **Vertical dashed line** = date of last Open-FF data set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14d39c-d880-4c79-b14c-0ddc436464da",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = wholeset[wholeset.change_type=='added']\n",
    "\n",
    "gb = t.groupby('date_changed',as_index=False).size()\n",
    "gb['cs'] = gb['size'].cumsum()\n",
    "gb = gb[['date_changed','cs']].set_index('date_changed')\n",
    "ax = gb.cs.plot(title='Number of new disclosures',\n",
    "               ylabel='cumulative disclosures', xlabel='date changed')\n",
    "\n",
    "gb = t[t.issues.str.len()>1].groupby('date_changed',as_index=False).size()\n",
    "gb['with_issues'] = gb['size'].cumsum()\n",
    "gb = gb[['date_changed','with_issues']].set_index('date_changed')\n",
    "ax = gb.with_issues.plot(ax=ax)\n",
    "\n",
    "\n",
    "ax.axvline(last_repo, color=\"green\", linestyle=\"dashed\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae398c-4578-4ec8-9ae7-b521120231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_whole = wholeset[wholeset.change_type=='added'].rename({'job_end_date':'job end date','date_changed':'date added',\n",
    "                            'change_type':'change type'},axis=1)\n",
    "# show_whole.TBWV = pd.to_numeric(show_whole.TBWV)\n",
    "try:\n",
    "    show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "              'TBWV','date added','has more than one','issues']].to_parquet(added_disc_fn)\n",
    "except:\n",
    "    print(f'Could not save to {added_disc_fn}')\n",
    "\n",
    "show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date added','has more than one','issues']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083babd9-0326-4f71-a088-ed315c81519d",
   "metadata": {},
   "source": [
    "### Removed disclosures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bb233-af53-4be5-acc0-25c65f04e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if removed_sum.sum()>0.5:\n",
    "    calplot.calplot(removed_sum, cmap='Spectral_r', yearlabel_kws={'fontname':'sans-serif'});\n",
    "else:\n",
    "    display(md('#### No removed disclosures found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bdd49-76d3-4358-af75-a6b1aa6d16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.geeksforgeeks.org/test-the-given-page-is-found-or-not-on-the-server-using-python/#\n",
    "import requests\n",
    "def url_ok(url):\n",
    "    # return True\n",
    "\t# exception block\n",
    "\ttry:\n",
    "\t\t# pass the url into\n",
    "\t\t# request.head\n",
    "\t\tresponse = requests.head(url)\n",
    "\t\t\n",
    "\t\t# check the status code\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\texcept requests.ConnectionError as e:\n",
    "\t\treturn e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35626e9a-f15e-40e7-b0cc-d229b20d9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_dfs_param(target_Did,target_api):\n",
    "    dfs = wholeset[wholeset.APINumber==target_api]\n",
    "    numdfs = len(dfs)\n",
    "    otherDids = {}\n",
    "    for i,row in dfs.iterrows():\n",
    "        if row.discID != target_Did:\n",
    "            otherDids[row.discID] = row.date_changed\n",
    "    return otherDids    \n",
    "\n",
    "def other_has_different_discID(row,others):\n",
    "    has_different_dId = False\n",
    "    for dId in others:\n",
    "        if dId!=row.discID:\n",
    "            has_different_dId = True\n",
    "    return has_different_dId\n",
    "\n",
    "def other_is_likely_replacement(row,others):\n",
    "    replacementDid = None\n",
    "    for dId in others:\n",
    "        dtime = (row['date removed'] -  others[dId]).days\n",
    "        if abs(dtime)<31:\n",
    "            replacementDid = dId\n",
    "    return replacementDid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802b872-1e25-418d-9fa7-3027082301aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datelst = get_raw_list()\n",
    "old_rem = pd.read_parquet(removed_disc_fn)\n",
    "\n",
    "if removed_sum.sum()>0.5:   #0.5 to account for the 0.001 baseline\n",
    "    show_whole = wholeset[wholeset.change_type=='removed'].rename({'job_end_date':'job end date',\n",
    "                                                                   'date_changed':'date removed',\n",
    "                                                                   'change_type':'change type'},axis=1)\n",
    "    # ldiffs = []\n",
    "    # hmto = []\n",
    "    # for i,row in show_whole.iterrows():\n",
    "    #     if row['has more than one']:\n",
    "    #         others = get_other_dfs_param(row.discID,row.APINumber)\n",
    "    #         # print(f'How many others? {len(others.keys())}')\n",
    "    #         # Don't deal with well with too many or too few other disclosures\n",
    "    #         if len(others.keys())!=1:\n",
    "    #             ldiffs.append(-1)\n",
    "    #             hmto.append(f'others != 1; instead: {len(others.keys())}')\n",
    "    #             continue\n",
    "            \n",
    "    #         # make sure it is a different discID - otherwise it was added BEFORE removed\n",
    "    #         if not other_has_different_discID(row,others):\n",
    "    #             ldiffs.append(-1)\n",
    "    #             hmto.append('other was added BEFORE this removal')\n",
    "    #             continue  # ignore other, it was earlier and dropped\n",
    "\n",
    "    #         # make sure other is a likely REPLACEMENT - the change dates are similar\n",
    "    #         replacementDid = other_is_likely_replacement(row,others)\n",
    "    #         if not replacementDid:\n",
    "    #             ldiffs.append(-1)\n",
    "    #             hmto.append(\"other's change date was >30 days different\")\n",
    "    #             continue\n",
    "                \n",
    "    #         chg_date = row['date removed'].strftime(\"%Y-%m-%d\")\n",
    "    #         outfn = get_local_diff_fn(row.APINumber,chg_date)\n",
    "    #         url = get_diff_url(row.APINumber,chg_date)\n",
    "    #         if url_ok(url):  # don't remake this diff file if already uploaded\n",
    "    #             try: \n",
    "    #                 # # temp\n",
    "    #                 # ldiffs.append(0)\n",
    "    #                 # hmto.append(th.wrap_URL_in_html(url,'compare with new'))\n",
    "    #                 # continue\n",
    "\n",
    "    #                 ldiff_df = old_rem[(old_rem.APINumber==row.APINumber) & (old_rem['date removed']==row['date removed'])]['difflib size']\n",
    "    #                 if len(ldiff_df)>0: # the old df is usable\n",
    "    #                     ldiffs.append(ldiff_df.iloc[0])\n",
    "    #                     hmto.append(th.wrap_URL_in_html(url,'compare with new'))\n",
    "    #                     continue\n",
    "    #                 else:\n",
    "    #                     print(' -- regenerating ldiff...') # let it regenerate\n",
    "    #             except:\n",
    "    #                 print(f'something went wrong with already generated {row.APINumber}') \n",
    "\n",
    "    #         # make the diff file\n",
    "    #         try:\n",
    "    #             len_diff = PDF_like_compare(row.APINumber,row.discID,replacementDid,chg_date,datelst,outhtml=outfn)\n",
    "    #             ldiffs.append(len_diff)\n",
    "    #             hmto.append(th.wrap_URL_in_html(url,'compare with new'))\n",
    "    #         except:\n",
    "    #             ldiffs.append(-1)\n",
    "    #             hmto.append('error with diff process')\n",
    "    #     else:\n",
    "    #         ldiffs.append(-1)\n",
    "    #         hmto.append('')\n",
    "    # show_whole['difflib size'] = ldiffs\n",
    "    # show_whole['has more than one'] = hmto\n",
    "    show_whole.TBWV = show_whole.TBWV.astype('str')\n",
    "    # show_whole['difflib size'] = show_whole['difflib size'].astype('str')\n",
    "\n",
    "    show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "                'TBWV','date removed',\n",
    "                # 'has more than one','difflib size',\n",
    "                'issues']].to_parquet(removed_disc_fn)\n",
    "    \n",
    "    iShow(show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "                      'TBWV','date removed',\n",
    "                      # 'has more than one','difflib size',\n",
    "                      'issues']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1641db-e91f-4f0a-8e80-5d164a5a5a88",
   "metadata": {},
   "source": [
    "### Modified disclosures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d224ad-f887-4f59-89c0-6429b94cbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if changed_sum.sum()>0.5:  #0.5 to account for the 0.001 baseline\n",
    "    calplot.calplot(changed_sum, cmap='Spectral_r', yearlabel_kws={'fontname':'sans-serif'});\n",
    "else:\n",
    "    display(md('#### No modified disclosures detected'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d809105-05c5-4f84-a77a-5711206151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if changed_sum.sum()>0.5:\n",
    "    show_whole = wholeset[wholeset.change_type=='modified'].rename({'job_end_date':'job end date','date_changed':'date modified',\n",
    "                            'change_type':'change type'},axis=1)\n",
    "    show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date modified','has more than one','issues']].to_parquet(modified_disc_fn)\n",
    "    iShow(show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date modified','has more than one','issues']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069ef31-a598-4e01-b2ce-244cc411af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of *reported* issues\n",
    "def add_to_set(s, iset):\n",
    "    lst = s.split()\n",
    "    for i in lst:\n",
    "        iset.add(i)\n",
    "    return iset\n",
    "\n",
    "iset = set()\n",
    "for i, row in wholeset.iterrows():\n",
    "    iset = add_to_set(row.issues,iset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fedc77-887f-4f01-96a4-874adc97c5f8",
   "metadata": {},
   "source": [
    "### Issues list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c746d23-19ff-4f4f-a6b7-1534b3ea8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FF_issues.process_master_files as pmf\n",
    "pobj = pmf.Process_Master_Files()\n",
    "df = pobj.process_obj()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db158ddd-a2d6-46dd-9539-4160bf76a20b",
   "metadata": {},
   "source": [
    "#### Discloure-level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7168f5-7a23-4664-b9a0-e4485c10943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.Flag_id.str[0]=='d'\n",
    "c1 = df.Flag_id.isin(iset)\n",
    "t = df[c&c1].copy()\n",
    "t['flaw_link'] = t.Flag_id.map(lambda x: th.getFlawLink(x))\n",
    "t[['Title','flaw_link','Warning_level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f16ad-264c-489b-bbcc-3d8f58aec789",
   "metadata": {},
   "source": [
    "#### Record-level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84082326-55a7-4a36-94da-795ec25b0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.Flag_id.str[0]=='r'\n",
    "# c1 = df.Flag_id.isin(iset)\n",
    "t = df[c&c1].copy()\n",
    "t['flaw_link'] = t.Flag_id.map(lambda x: th.getFlawLink(x))\n",
    "t[['Title','flaw_link','Warning_level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134fccd-f9b8-4ae8-a811-dd50ee62ffbf",
   "metadata": {},
   "source": [
    "## New CASNumber : IngredientName pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17793bc7-f52f-4aab-aae5-b3204a018ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there new casing?\n",
    "repo_casing = fh.get_casing_df()\n",
    "changed_casing = pd.DataFrame(casing,columns=['CASNumber','IngredientName'])\n",
    "mg = pd.merge(changed_casing,repo_casing,on=['CASNumber','IngredientName'], how='left',indicator=True)\n",
    "mg[mg._merge=='left_only'][['CASNumber','IngredientName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e5ca0-3fc4-48d6-8970-28d7284f29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there new Operators?\n",
    "repo_companies = fh.get_company_df()\n",
    "complst = repo_companies.rawName.tolist()\n",
    "newcomp = []\n",
    "for op in operator:\n",
    "    if not op in complst:\n",
    "        newcomp.append(op)\n",
    "if len(newcomp)> 0:\n",
    "    display(md('## New Operator names detected'))\n",
    "    newcomp.sort()\n",
    "    for item in newcomp:\n",
    "        display(md(f'##### {item}'))\n",
    "else:\n",
    "    display(md('### No new operator names detected'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52667e22-147d-43d6-bfc5-cf3f9a2f6afb",
   "metadata": {},
   "source": [
    "___\n",
    "## Watch list\n",
    "Wells with changed disclosures that were previously detected with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f559e-d70f-4aa0-90de-0ef15557cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522116ef-dc1b-4679-8526-4cfc3275f9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
