{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783b434-731f-438d-bf53-e09a653af90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'c:/MyDocs/integrated/') # adjust to your setup\n",
    "\n",
    "%run \"catalog_support.py\" \n",
    "\n",
    "Numdays = 4\n",
    "\n",
    "showHeader('Raw Disclosures',line2=f'{Numdays} days of FracFocus changes',use_remote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01ba08-8fd4-4556-a835-28674be213ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "last_repo = datetime.datetime(year=2024,month=10,day=8)\n",
    "today = datetime.datetime.today()\n",
    "edate = today - datetime.timedelta(days=Numdays)\n",
    "# print('earlist shown',edate)\n",
    "\n",
    "daily_status_folder = r\"G:\\My Drive\\webshare\\daily_status\"\n",
    "added_disc_fn = os.path.join(daily_status_folder,'added_disc_df.parquet')\n",
    "removed_disc_fn = os.path.join(daily_status_folder,'removed_disc_df.parquet')\n",
    "modified_disc_fn = os.path.join(daily_status_folder,'modified_disc_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56e618-e533-4c3f-b28c-3525036f27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import openFF.common.text_handlers as th\n",
    "arc_dir = r\"C:\\MyDocs\\integrated\\openFF_archive\\diff_dicts\"\n",
    "\n",
    "diff_fns = os.listdir(arc_dir)\n",
    "download_dates = []\n",
    "added = []\n",
    "changed = []\n",
    "removed = []\n",
    "casing = set()\n",
    "operator = set()\n",
    "\n",
    "\n",
    "for fn in diff_fns:\n",
    "    tdate = datetime.datetime(int(fn[10:14]),int(fn[15:17]),int(fn[18:20]))\n",
    "    # print(tdate)\n",
    "    if tdate>= edate:\n",
    "        download_dates.append(fn[10:20])\n",
    "        with open(os.path.join(arc_dir,fn),'rb') as f:\n",
    "            diff_dic = pickle.load(f)\n",
    "        if len(diff_dic['removed_disc'])>0:\n",
    "            t = diff_dic['removed_disc'].copy()\n",
    "            # print(t.head())\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'removed'\n",
    "            removed.append(t)\n",
    "        if len(diff_dic['added_disc'])>0:\n",
    "            t = diff_dic['added_disc'].copy()\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'added'\n",
    "            added.append(t)\n",
    "        if len(diff_dic['changed_disc'])>0:\n",
    "            t = diff_dic['changed_disc'].copy()\n",
    "            t['date_changed'] = tdate\n",
    "            t['change_type'] = 'modified'            \n",
    "            changed.append(t)\n",
    "        if len(diff_dic['casing'])>0:\n",
    "            for item in diff_dic['casing']:\n",
    "                # print(item)\n",
    "                if item[1]==None: ig = ''\n",
    "                else: ig = item[1].strip().lower()\n",
    "                tup = (item[0],ig)\n",
    "                casing.add(tup)\n",
    "        if len(diff_dic['OperatorName'])>0:\n",
    "            for item in diff_dic['OperatorName']:\n",
    "                operator.add(item)\n",
    "\n",
    "alllists = added + changed + removed\n",
    "wholeset = pd.concat(alllists,sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d462b-e9a5-4ce5-a83b-ca174872603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_added = wholeset[wholeset.change_type=='added'].groupby('date_changed').size()\n",
    "added_sum = gb_added.resample(\"D\").sum()\n",
    "added_sum = added_sum+.001 # to distinguish between zeros and no data\n",
    "\n",
    "gb_removed = wholeset[wholeset.change_type=='removed'].groupby('date_changed').size()\n",
    "removed_sum = gb_removed.resample(\"D\").sum()\n",
    "removed_sum = removed_sum+0.001\n",
    "\n",
    "gb_changed = wholeset[wholeset.change_type=='modified'].groupby('date_changed').size()\n",
    "changed_sum = gb_changed.resample(\"D\").sum()\n",
    "changed_sum = changed_sum+0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abeb241-a3ea-4601-98df-3963c071f3da",
   "metadata": {},
   "source": [
    "### set up the difflib outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382bc8d-d799-4ec9-ba0e-f774862d3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "archive_folder = r\"D:\\openFF_archive\\raw_dataframes\"\n",
    "archive_folder = r\"C:\\MyDocs\\integrated\\openFF_archive\\raw_dataframes\"\n",
    "\n",
    "def get_raw_list():\n",
    "    alst = os.listdir(archive_folder)\n",
    "    dates = []\n",
    "    for f in alst:\n",
    "        dates.append(f[7:17])\n",
    "    return dates\n",
    "    \n",
    "def get_early_late_fn(change_date,datelst):\n",
    "    latefn = os.path.join(archive_folder,'raw_df_'+change_date+'.parquet')\n",
    "    idx = datelst.index(change_date)\n",
    "    earlyfn = os.path.join(archive_folder,'raw_df_'+datelst[idx-1]+'.parquet')\n",
    "    # print(earlyfn+'\\n'+latefn)\n",
    "    return earlyfn,latefn\n",
    "\n",
    "def get_comparison_dfs(apinum,earlyfn, latefn):\n",
    "    filt = [('APINumber','==',apinum)]\n",
    "    edf = pd.read_parquet(earlyfn,filters=filt)\n",
    "    ldf = pd.read_parquet(latefn,filters=filt)\n",
    "    return edf,ldf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30257dc-6909-40ab-8ed7-8d427e7bfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_difflink_from_df(api,change_date):\n",
    "    return os.path.join(r\"G:\\My Drive\\webshare\\daily_status\\diff_files\",api+'_'+change_date+'.html')\n",
    "\n",
    "def get_url_from_df(api,change_date):\n",
    "    return \"https://storage.googleapis.com/open-ff-browser/difflib/\",api+'_'+change_date+'.html'\n",
    "\n",
    "def make_logistics_summary(ddf):\n",
    "    # assumes single disclosure in ddf\n",
    "    # start with meta or logistics data, so all is available in any line\n",
    "    jsd = ddf.JobStartDate.iloc[0]\n",
    "    jed = ddf.JobEndDate.iloc[0]\n",
    "    state = ddf.StateName.iloc[0]\n",
    "    county = ddf.CountyName.iloc[0]\n",
    "    api = ddf.APINumber.iloc[0]\n",
    "    operator = ddf.OperatorName.iloc[0]\n",
    "    wellname = ddf.WellName.iloc[0]\n",
    "    lat = ddf.Latitude.iloc[0]\n",
    "    lon = ddf.Longitude.iloc[0]\n",
    "    datum = ddf.Projection.iloc[0]\n",
    "    fed = ddf.FederalWell.iloc[0]\n",
    "    indian = ddf.IndianWell.iloc[0]\n",
    "    tvd = ddf.TVD.iloc[0]\n",
    "    tbwv = ddf.TotalBaseWaterVolume.iloc[0]\n",
    "    tbnwv = ddf.TotalBaseNonWaterVolume.iloc[0]\n",
    "    did = ddf.DisclosureId.iloc[0]\n",
    "    ffv = ddf.FFVersion.iloc[0]\n",
    "\n",
    "    s=  '\\n**Logistics Section** \\n'\n",
    "    s+= f'APINumber: {api}\\n'\n",
    "    s+= f'Job Start Date: {jsd}\\n'\n",
    "    s+= f'Job End Date: {jed}\\n'\n",
    "    s+= f'State: {state}\\n'\n",
    "    s+= f'County: {county}\\n'\n",
    "    s+= f'Operator: {operator}\\n'\n",
    "    s+= f'Well Name: {wellname}\\n'\n",
    "    s+= f'Latitude: {lat}  Longitude: {lon}\\n'\n",
    "    s+= f'Projection: {datum}\\n'\n",
    "    s+= f'Federal: {fed}   Indian: {indian}\\n'\n",
    "    s+= f'Total Vertical Depth: {tvd}\\n'\n",
    "    s+= f'Carrier water volume: {tbwv}\\n'\n",
    "    s+= f'Vol of non-water carrier: {tbnwv}\\n'\n",
    "    s+= f'FF Version: {ffv}\\n'\n",
    "    s+= f'DisclosureId: {did}\\n'\n",
    "\n",
    "    return s\n",
    "\n",
    "def make_products_summary(ddf):\n",
    "    ###  TradeName products\n",
    "    ddf.CASNumber = ddf.CASNumber.fillna('--empty--')\n",
    "    ddf = ddf.sort_values(['TradeName','CASNumber'])\n",
    "    gb = ddf.groupby('TradeName')['CASNumber'].apply(set).reset_index()\n",
    "    s= '\\n**Products List Section with components**\\n'\n",
    "    for i,row in gb.iterrows():\n",
    "        s+= f'Name: {row.TradeName}\\n'\n",
    "        lst = list(row.CASNumber)\n",
    "        lst.sort()\n",
    "        for cas in lst:\n",
    "            s+= f'      - {cas}\\n'\n",
    "    return s\n",
    "    # tn = ddf.TradeName.unique().tolist()\n",
    "    # tn.sort()\n",
    "    # for i in tn:\n",
    "    #     s+= f'Name: {i}\\n'\n",
    "\n",
    "def make_record_count_summary(ddf):\n",
    "    ###  Number of records\n",
    "    cn = ddf.CASNumber.notna()&ddf.PercentHFJob.notna()\n",
    "    s= '\\n**Number of records Section**\\n'\n",
    "    s+= f'Total num records: {len(ddf)}\\n'\n",
    "    s+= f'Record with chem info: {len(ddf[cn])}\\n'\n",
    "    return s\n",
    "\n",
    "def make_num_duplicates(ddf):\n",
    "    cn = ddf.CASNumber.notna()&ddf.IngredientName.notna()&ddf.PercentHighAdditive.notna()&ddf.PercentHFJob.notna()\n",
    "    cdup = ddf[['CASNumber','IngredientName','PercentHighAdditive','PercentHFJob','MassIngredient']].duplicated()\n",
    "    s = '\\n**Number of duplicate records**\\n'\n",
    "    t = ddf[cn&cdup].copy()\n",
    "    s += f'Number of duplicate records: {len(t)}\\n'\n",
    "    # for i,row in t.iterrows():\n",
    "    #     s += f'duplicated record: {row.CASNumber}, {row.IngredientName}, {row.PercentHFJob}\\n'\n",
    "    return s\n",
    "\n",
    "def make_chem_list(ddf):\n",
    "    #  List of UNIQUE CAS/ING\n",
    "    s = '\\n**Unique CASNumber/IngredientName pairs**\\n'\n",
    "    t = ddf.groupby(['CASNumber','IngredientName'],as_index=False).size().rename({'size':'num_recs'},axis=1)\n",
    "    t = t.sort_values(['CASNumber','IngredientName'])\n",
    "    for i,row in t.iterrows():\n",
    "        s += f'CAS: {row.CASNumber}, Name: {row.IngredientName}, How many: {row.num_recs}\\n'\n",
    "    return s\n",
    "\n",
    "def make_chem_data_list(ddf):\n",
    "    # list of main components of a data record - only those with CASNumber or IngredientName\n",
    "    s = '\\n**record data**\\n\\n'\n",
    "    sortby_order = ['TradeName','Purpose','Supplier','PercentHFJob','CASNumber','IngredientName']\n",
    "    # t = ddf.groupby(['CASNumber','IngredientName'],as_index=False).size().rename({'size':'num_recs'},axis=1)\n",
    "    t = ddf.sort_values(sortby_order)\n",
    "    for i,row in t.iterrows():\n",
    "        s += f'{row.TradeName};{row.Purpose};{row.Supplier}\\n\\t{row.CASNumber}\\n\\t{row.IngredientName}\\n\\t{row.PercentHighAdditive};{row.PercentHFJob};{row.MassIngredient}\\n'\n",
    "    return s\n",
    "    \n",
    "# def make_diff_str(str1,str2,section='logistics'):\n",
    "#     diff = difflib.context_diff(str1, str2,n=20,fromfile='', tofile='')\n",
    "#     s = f'\\n\\n<<{section}>>\\n'\n",
    "#     for line in diff:\n",
    "#         s+= line\n",
    "#     # print(f'*****   DIFF FILE FOLLOWS *****\\n{s}')\n",
    "#     return s\n",
    "\n",
    "def make_html_diff(str1,str2):\n",
    "    HD = difflib.HtmlDiff(wrapcolumn=80)\n",
    "    return HD.make_file(str1, str2,fromdesc='original', todesc='modified')\n",
    "\n",
    "def get_diff_size(str1,str2):\n",
    "    diff = difflib.context_diff(str1, str2,fromfile='original', tofile='modified')\n",
    "    s = ''\n",
    "    for line in diff:\n",
    "        s+= line\n",
    "    return len(s)\n",
    "\n",
    "# def comparison_alerts(edf,ldf):\n",
    "    \n",
    "    \n",
    "def PDF_like_compare(apinumber,change_date,datelst,outhtml):\n",
    "    print(f'working on {apinumber} {change_date}')\n",
    "    earlyfn,latefn = get_early_late_fn(change_date=change_date,datelst=datelst)\n",
    "    edf,ldf = get_comparison_dfs(apinumber,earlyfn,latefn)\n",
    "    str1 = f'Archive file :\\n\\t{earlyfn}\\n'\n",
    "    str1 += make_logistics_summary(edf)\n",
    "    str1 += make_products_summary(edf)\n",
    "    str1 += make_record_count_summary(edf)\n",
    "    str1 += make_num_duplicates(edf)\n",
    "    str1 += make_chem_list(edf)\n",
    "    str1 += make_chem_data_list(edf)\n",
    "    lst1 = str1.split('\\n')\n",
    "    str2 = f'Archive file :\\n\\t{latefn}\\n'\n",
    "    str2 += make_logistics_summary(ldf)\n",
    "    str2 += make_products_summary(ldf)\n",
    "    str2 += make_record_count_summary(ldf)\n",
    "    str2 += make_num_duplicates(ldf)\n",
    "    str2 += make_chem_list(ldf)\n",
    "    str2 +=make_chem_data_list(ldf)\n",
    "    lst2 = str2.split('\\n')\n",
    "    # print(str1)\n",
    "    out  = make_html_diff(lst1,lst2)\n",
    "    # outhtml = get_difflink_from_df(apinumber)\n",
    "    with open(outhtml,'w') as f:\n",
    "        f.write(out)\n",
    "    return(get_diff_size(lst1,lst2))\n",
    "    \n",
    "\n",
    "# PDF_like_compare(edf,ldf,apinum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cd6de-13bc-46ff-8a28-dc723e3fb4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aac9700a-b021-4f7a-8fe3-21cad0e9b704",
   "metadata": {},
   "source": [
    "## Downloads from FracFocus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ef4ae-b390-4270-b074-22da5b7e376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset['job_end_date'] = wholeset.JobEndDate.str.split().str[0]\n",
    "wholeset['job_end_date'] = pd.to_datetime(wholeset.job_end_date,format=\"%m/%d/%Y\")\n",
    "wholeset['FF_disc'] = wholeset.apply(lambda x: th.getFFLink(x), axis=1)\n",
    "wholeset['disc_link'] = wholeset.apply(lambda x: th.getDisclosureLink(APINumber=x.APINumber,\n",
    "                                                                      disclosureid=x.discID,\n",
    "                                                                      text_to_show='Open-FF disclosure',\n",
    "                                                                      use_remote=True,\n",
    "                                                                      check_if_exists=False), axis=1)\n",
    "cond = (wholeset.date_changed>=last_repo)&(wholeset.change_type!='removed')\n",
    "wholeset.disc_link = np.where(cond,' ',wholeset.disc_link)\n",
    "wholeset['TBWV'] = wholeset.TotalBaseWaterVolume.astype('float64')\n",
    "wholeset['TBWV'] = wholeset.TBWV.map(lambda x: th.round_sig(x,5))\n",
    "wholeset['TBWV'] = wholeset.TBWV.fillna(' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8060d8-bf7e-4a55-a502-41f4fa1da722",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset.change_type.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd7ba5-f204-4a4c-aed1-1c2a8a654687",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeset['has more than one'] = np.where(wholeset.APINumber.duplicated(keep=False),'APINumber dupe','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4737e-af11-4f58-bc3e-127630e4b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curated disclosures, get detected flaws\n",
    "\n",
    "DiDs = wholeset.discID.unique().tolist()\n",
    "rec_iss = pd.read_parquet(os.path.join(hndl.curr_repo_dir,'record_issues.parquet'),columns=['r_flags','reckey'])\n",
    "reck = pd.read_parquet(os.path.join(hndl.curr_repo_pkl_dir,'chemrecs.parquet'),columns=['DisclosureId','reckey'])\n",
    "mg = pd.merge(rec_iss,reck,on='reckey',how='left')\n",
    "gb = mg[mg.DisclosureId.isin(DiDs)].groupby('DisclosureId',as_index=False)['r_flags'].apply(set)\n",
    "\n",
    "def str_from_set(x):\n",
    "    s = ''\n",
    "    for item in x:\n",
    "        s += item +' '\n",
    "    return s\n",
    "\n",
    "gb['recstr'] = gb.r_flags.map(lambda x: str_from_set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba9995-b518-47fc-b64b-e9184ef8e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_iss = pd.read_parquet(os.path.join(hndl.curr_repo_dir,'disclosure_issues.parquet'),columns=['d_flags','DisclosureId'])\n",
    "dis_iss = dis_iss[dis_iss.DisclosureId.isin(DiDs)]\n",
    "gb = gb.merge(dis_iss,on='DisclosureId',how='outer')\n",
    "gb = gb.fillna('')\n",
    "gb['issues'] = gb.d_flags +' '+gb.recstr\n",
    "wholeset = wholeset.merge(gb[['DisclosureId','issues']],left_on='discID',right_on='DisclosureId',how='left')\n",
    "wholeset.issues = wholeset.issues.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fe2b0-2319-4ffd-8a20-44edafba35c9",
   "metadata": {},
   "source": [
    "## watch list summary\n",
    "See bottom of page for whole list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0f5d9-e25a-4300-bc97-75e500f58d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://raw.githubusercontent.com/gwallison/FF_issues/master/watch_list.csv'\n",
    "# wdf = pd.read_csv(url,dtype = {'APINumber':'str'})\n",
    "url = 'https://raw.githubusercontent.com/gwallison/FF_issues/master/watch_list_master.parquet'\n",
    "wdf = pd.read_parquet(url)\n",
    "wdf = wdf.rename({'DisclosureId':'wl_DisclosureId'},axis=1)\n",
    "# wdf.date_entered = pd.to_datetime(wdf.date_entered,format='%m/%d/%y')\n",
    "# wdf.FF_report_date = pd.to_datetime(wdf.FF_report_date,format='%m/%d/%y')\n",
    "# wdf.Blog_date = pd.to_datetime(wdf.Blog_date,format='%m/%d/%y')\n",
    "# wdf.FF_updates = pd.to_datetime(wdf.FF_updates,format='%m/%d/%y')\n",
    "\n",
    "apis = wdf.APINumber.unique().tolist()\n",
    "\n",
    "watchlist_found = pd.merge(wdf,wholeset,on='APINumber',how='inner')\n",
    "watchlist_found[['wl_name','change_type']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67e911-5989-4a29-9302-ae8b013d0bb6",
   "metadata": {},
   "source": [
    "## Pattern of new disclosure additions\n",
    "These disclosures are detected as new because their `DisclosureId` number hasn't been in the database before.  Note that it is possible that they are a new version of a previously published disclosure; sometimes operators change disclosures by removing the old one from FracFocus and creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d9ce-4d0f-4164-be4c-aee022002f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(download_dates)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import calplot\n",
    "calplot.calplot(added_sum,  cmap='Spectral_r');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79219256-de05-4fb5-9909-ecf97da76da5",
   "metadata": {},
   "source": [
    "- **Blue line** = Cumulative new disclosures added (my include replacements for removed disclosures)\n",
    "- **Orange line** = New disclosures with detected issues\n",
    "- **Vertical dashed line** = date of last Open-FF data set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14d39c-d880-4c79-b14c-0ddc436464da",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = wholeset[wholeset.change_type=='added']\n",
    "\n",
    "gb = t.groupby('date_changed',as_index=False).size()\n",
    "gb['cs'] = gb['size'].cumsum()\n",
    "gb = gb[['date_changed','cs']].set_index('date_changed')\n",
    "ax = gb.cs.plot(title='Number of new disclosures',\n",
    "               ylabel='cumulative disclosures', xlabel='date changed')\n",
    "\n",
    "gb = t[t.issues.str.len()>1].groupby('date_changed',as_index=False).size()\n",
    "gb['with_issues'] = gb['size'].cumsum()\n",
    "gb = gb[['date_changed','with_issues']].set_index('date_changed')\n",
    "ax = gb.with_issues.plot(ax=ax)\n",
    "\n",
    "\n",
    "ax.axvline(last_repo, color=\"green\", linestyle=\"dashed\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae398c-4578-4ec8-9ae7-b521120231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_whole = wholeset[wholeset.change_type=='added'].rename({'job_end_date':'job end date','date_changed':'date added',\n",
    "                            'change_type':'change type'},axis=1)\n",
    "show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date added','has more than one','issues']].to_parquet(added_disc_fn)\n",
    "\n",
    "show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date added','has more than one','issues']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083babd9-0326-4f71-a088-ed315c81519d",
   "metadata": {},
   "source": [
    "### Removed disclosures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bb233-af53-4be5-acc0-25c65f04e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if removed_sum.sum()>0.5:\n",
    "    calplot.calplot(removed_sum, cmap='Spectral_r');\n",
    "else:\n",
    "    display(md('#### No removed disclosures found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e4bdd49-76d3-4358-af75-a6b1aa6d16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.geeksforgeeks.org/test-the-given-page-is-found-or-not-on-the-server-using-python/#\n",
    "import requests\n",
    "def url_ok(url):\n",
    "\t# exception block\n",
    "\ttry:\n",
    "\t\t# pass the url into\n",
    "\t\t# request.head\n",
    "\t\tresponse = requests.head(url)\n",
    "\t\t\n",
    "\t\t# check the status code\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\texcept requests.ConnectionError as e:\n",
    "\t\treturn e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802b872-1e25-418d-9fa7-3027082301aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datelst = get_raw_list()\n",
    "old_rem = pd.read_parquet(removed_disc_fn)\n",
    "\n",
    "if removed_sum.sum()>0.5:   #0.5 to account for the 0.001 baseline\n",
    "    show_whole = wholeset[wholeset.change_type=='removed'].rename({'job_end_date':'job end date',\n",
    "                                                                   'date_changed':'date removed',\n",
    "                                                                   'change_type':'change type'},axis=1)\n",
    "    # show_whole['diff_fn'] = show_whole.APINumber.map(lambda x: get_difflink_from_df(x))\n",
    "    ldiffs = []\n",
    "    hmto = []\n",
    "    for i,row in show_whole.iterrows():\n",
    "        if row['has more than one']:\n",
    "            chg_date = row['date removed'].strftime(\"%Y-%m-%d\")\n",
    "            outfn = get_difflink_from_df(row.APINumber,chg_date)\n",
    "            url = get_url_from_df(row.APINumber,chg_date)\n",
    "            if url_ok(url):  # don't remake this diff file if already uploaded\n",
    "                try: \n",
    "                    ldiff_df = old_rem[(old_rem.APINumber==row.APINumber) & (old_rem['date removed']==row['date removed'])]['difflib size']\n",
    "                    if len(ldiff_df)>0: # the old df is usable\n",
    "                        ldiffs.append(ldiff_df.iloc[0])\n",
    "                        hmto.append(th.wrap_URL_in_html(outfn,'compare with new'))\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(' -- regenerating...') # let it regenerate\n",
    "                except:\n",
    "                    print(f'something went wrong with already generated {row.APINumber}') \n",
    "\n",
    "            # make the diff file\n",
    "            try:\n",
    "                len_diff = PDF_like_compare(row.APINumber,chg_date,datelst,outhtml=outfn)\n",
    "                ldiffs.append(len_diff)\n",
    "                hmto.append(th.wrap_URL_in_html(url,'compare with new'))\n",
    "            except:\n",
    "                ldiffs.append(-1)\n",
    "                hmto.append('error with diff process')\n",
    "        else:\n",
    "            ldiffs.append(-1)\n",
    "            hmto.append('')\n",
    "    show_whole['difflib size'] = ldiffs\n",
    "    show_whole['has more than one'] = hmto\n",
    "    show_whole.TBWV = show_whole.TBWV.astype('str')\n",
    "    show_whole['difflib size'] = show_whole['difflib size'].astype('str')\n",
    "\n",
    "    show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date removed','has more than one','difflib size','issues']].to_parquet(removed_disc_fn)\n",
    "    \n",
    "    iShow(show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date removed','has more than one','difflib size','issues']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1641db-e91f-4f0a-8e80-5d164a5a5a88",
   "metadata": {},
   "source": [
    "### Modified disclosures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d224ad-f887-4f59-89c0-6429b94cbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if changed_sum.sum()>0.5:  #0.5 to account for the 0.001 baseline\n",
    "    calplot.calplot(changed_sum, cmap='Spectral_r');\n",
    "else:\n",
    "    display(md('#### No modified disclosures detected'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d809105-05c5-4f84-a77a-5711206151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if changed_sum.sum()>0.5:\n",
    "    show_whole = wholeset[wholeset.change_type=='modified'].rename({'job_end_date':'job end date','date_changed':'date modified',\n",
    "                            'change_type':'change type'},axis=1)\n",
    "    show_whole[['APINumber','FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date modified','has more than one','issues']].to_parquet(modified_disc_fn)\n",
    "    iShow(show_whole[['FF_disc','disc_link','job end date','StateName','CountyName','OperatorName',\n",
    "          'TBWV','date modified','has more than one','issues']].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069ef31-a598-4e01-b2ce-244cc411af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of *reported* issues\n",
    "def add_to_set(s, iset):\n",
    "    lst = s.split()\n",
    "    for i in lst:\n",
    "        iset.add(i)\n",
    "    return iset\n",
    "\n",
    "iset = set()\n",
    "for i, row in wholeset.iterrows():\n",
    "    iset = add_to_set(row.issues,iset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fedc77-887f-4f01-96a4-874adc97c5f8",
   "metadata": {},
   "source": [
    "### Issues list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c746d23-19ff-4f4f-a6b7-1534b3ea8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FF_issues.process_master_files as pmf\n",
    "pobj = pmf.Process_Master_Files()\n",
    "df = pobj.process_obj()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db158ddd-a2d6-46dd-9539-4160bf76a20b",
   "metadata": {},
   "source": [
    "#### Discloure-level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7168f5-7a23-4664-b9a0-e4485c10943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.Flag_id.str[0]=='d'\n",
    "c1 = df.Flag_id.isin(iset)\n",
    "t = df[c&c1].copy()\n",
    "t['flaw_link'] = t.Flag_id.map(lambda x: th.getFlawLink(x))\n",
    "t[['Title','flaw_link','Warning_level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f16ad-264c-489b-bbcc-3d8f58aec789",
   "metadata": {},
   "source": [
    "#### Record-level Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84082326-55a7-4a36-94da-795ec25b0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.Flag_id.str[0]=='r'\n",
    "# c1 = df.Flag_id.isin(iset)\n",
    "t = df[c&c1].copy()\n",
    "t['flaw_link'] = t.Flag_id.map(lambda x: th.getFlawLink(x))\n",
    "t[['Title','flaw_link','Warning_level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134fccd-f9b8-4ae8-a811-dd50ee62ffbf",
   "metadata": {},
   "source": [
    "## New CASNumber : IngredientName pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17793bc7-f52f-4aab-aae5-b3204a018ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there new casing?\n",
    "repo_casing = fh.get_casing_df()\n",
    "changed_casing = pd.DataFrame(casing,columns=['CASNumber','IngredientName'])\n",
    "mg = pd.merge(changed_casing,repo_casing,on=['CASNumber','IngredientName'], how='left',indicator=True)\n",
    "mg[mg._merge=='left_only'][['CASNumber','IngredientName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e5ca0-3fc4-48d6-8970-28d7284f29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there new Operators?\n",
    "repo_companies = fh.get_company_df()\n",
    "complst = repo_companies.rawName.tolist()\n",
    "newcomp = []\n",
    "for op in operator:\n",
    "    if not op in complst:\n",
    "        newcomp.append(op)\n",
    "if len(newcomp)> 0:\n",
    "    display(md('## New Operator names detected'))\n",
    "    newcomp.sort()\n",
    "    for item in newcomp:\n",
    "        display(md(f'##### {item}'))\n",
    "else:\n",
    "    display(md('### No new operator names detected'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52667e22-147d-43d6-bfc5-cf3f9a2f6afb",
   "metadata": {},
   "source": [
    "___\n",
    "## Watch list\n",
    "Wells with changed disclosures that were previously detected with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f559e-d70f-4aa0-90de-0ef15557cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist_found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2776d593-c3b3-4ae5-a4d3-7673d957be24",
   "metadata": {},
   "source": [
    "## Monitor ECMC disclosure list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ff74a-b773-4b2c-b032-33374a41099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ecmc.state.co.us/depot/Stats/ChemicalDisclosures/Index/Locations\"\n",
    "t = pd.read_html(url)\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410f1ce-72f3-4cf0-8582-8349e2100864",
   "metadata": {},
   "source": [
    "## Monitoring sand-dominated disclosures\n",
    "\n",
    "The following code is used to keep a running list (in a dataframe) of the disclosure that have greater than 50% sand.  This list will be used to monitor the success of efforts to inform companies of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779124c-173d-4dd7-8397-e1275626bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openFF.build.core.Bulk_data_reader as bdr\n",
    "rff = bdr.Read_FF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4b17d-23ee-4940-9c99-c735d355d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "rawdir = r\"C:\\MyDocs\\integrated\\openFF_archive\\raw_dataframes\"\n",
    "dirlist = os.listdir(rawdir)\n",
    "lastraw = dirlist[-1]\n",
    "print(f'Using {lastraw} as data source')\n",
    "rawdf = pd.read_parquet(os.path.join(rawdir,lastraw))\n",
    "rawdate = datetime.datetime(int(lastraw[7:11]),int(lastraw[12:14]),int(lastraw[15:17]))\n",
    "# print(rawdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd62c99-a081-45ca-a419-f7384f7da86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdf.PercentHFJob = pd.to_numeric(rawdf.PercentHFJob)\n",
    "rawdf = rff.make_date_fields(rawdf)\n",
    "gb = rawdf[rawdf.CASNumber=='14808-60-7'].groupby('DisclosureId',as_index=False).PercentHFJob.sum()\n",
    "gb2 = rawdf.groupby('DisclosureId',as_index=False)[['APINumber','OperatorName','StateName',\n",
    "                                                    'CountyName','JobEndDate','date','year']].first()\n",
    "mg = pd.merge(gb,gb2,on='DisclosureId',how='inner')\n",
    "\n",
    "sand_disc = mg[mg.PercentHFJob>50].DisclosureId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa2ad8-b294-4b5d-a961-8f1c249da65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = r\"G:\\My Drive\\webshare\\daily_status\\sand_monitoring_df.parquet\"\n",
    "try:\n",
    "    master = pd.read_parquet(fn)\n",
    "except:\n",
    "    master = pd.DataFrame({'DisclosureId':[],'added_date':[],'removed_date':[]})\n",
    "together = pd.merge(master,pd.DataFrame({'DisclosureId':sand_disc}),\n",
    "                    on='DisclosureId',how='outer',indicator=True)\n",
    "# now evaluate for new and removed\n",
    "removed = together[together._merge=='left_only'].DisclosureId.tolist()\n",
    "added = together[together._merge=='right_only'].DisclosureId.tolist()\n",
    "\n",
    "# For removed, make \"removed_date\" equal to rawdate\n",
    "c1 = master.DisclosureId.isin(removed)\n",
    "c2 = master.removed_date.isna()\n",
    "print(f'Number of disclosures removed from sand dominated list: {(c1&c2).sum()}')\n",
    "master['removed_date'] = np.where(c1&c2,rawdate,master.removed_date)\n",
    "\n",
    "# For added, get the other columns from mg and concatenate, then put in date_added\n",
    "added_df = mg[mg.DisclosureId.isin(added)].copy()\n",
    "print(f'Number of disclosures added as sand-dominated: {len(added_df)}')\n",
    "added_df['added_date'] = rawdate\n",
    "condf = pd.concat([master,added_df],sort=True)\n",
    "condf.to_parquet(fn)\n",
    "c3 = condf.removed_date.isna()\n",
    "print(f'Total number of sand dominated disclosures still on the list: {len(condf[c3])}')\n",
    "print(f'Updated file saved to {fn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522116ef-dc1b-4679-8526-4cfc3275f9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
