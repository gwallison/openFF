{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff4ea28-23c5-4b36-8937-54febce3d74c",
   "metadata": {},
   "source": [
    "# Transition to FracFocus version 4\n",
    "\n",
    "In Dec 2023, FracFocus changed the format (and to a small degree the content) of the bulk download.\n",
    "\n",
    "These changes necessitate some changes in Open-FF.  The following code creates the bridge between the old and new formats and describes the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3caee-0f0b-4c65-a992-d315099fe295",
   "metadata": {},
   "source": [
    "## Transforming the 'upload_dates' file\n",
    "One large change in FF was to stop using `UploadKey` as the primary key for disclosures (which had been stabel for at least 5 years) and replace it with a field named `DisclosureId`.  Even though the two are of the same format, etc., they are not same values for a given disclosure.  Because no bulk download has both fields, we must create a map of old to new keys if we are to continue to use the archived data (now 5 years of it).  \n",
    "\n",
    "In addition, crossing the transition can be tricky if we want to be able to identify \"new\" disclosures.  The plan to accomplish this is:\n",
    "1. use the new archive (Dec 4, 2023) to create a new baseline upload_dates file.  \n",
    "1. take the old archive (Nov 25, date of last full repo) and remove duplicates (but keep one - it will be connected to the DisclosureId) to make old_df.  Those duplicates will be based on: ['APINumber','JobEndDate','JobStartDate','OperatorName','TotalBaseWaterVolume']\n",
    "1. use old_df to connect `UploadKey`s to the `DisclosureId`.  This should keep all `DisclosureId`s (even the duplicates (though they will point to the same `UploadKey`).  \n",
    "1. use the old repo to populate the publish date in the new baseline update_dates file.  \n",
    "1. I probably want to add any `UploadKey`s that are not in the new baseline to make it backward compatible. `DisclosureId` will be NaN.\n",
    "1. Remove the disclosures in the baseline that have ONLY a `DisclosureId` - these are the legitimately new disclosures and will be added in the next build.\n",
    "\n",
    "These changes will be made locally since they only need to be applied once.  I won't pollute the github space with these data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8725bd-fc54-47d4-a13f-812feb37cda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "old_fn = r\"C:\\MyDocs\\OpenFF\\src\\openFF-integrated\\tmp\\all_meta\\ff_archive_meta_2023-11-25.parquet\"\n",
    "new_fn = r\"C:\\MyDocs\\OpenFF\\src\\openFF-integrated\\tmp\\all_meta\\ff_archive_meta_2023-12-04.parquet\"\n",
    "# see \"C:\\MyDocs\\OpenFF\\src\\openFF-integrated\\build_meta.ipynb\" for how these summary files were created\n",
    "\n",
    "old_upload_dates_fn = r\"C:\\MyDocs\\integrated\\repos\\openFF_data_2023_11_25\\curation_files\\upload_dates.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb50d55-47d6-4c7f-b88b-b1693a1a51e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 1 and 2\n",
    "newdf = pd.read_parquet(new_fn)\n",
    "olddf = pd.read_parquet(old_fn)\n",
    "# remove duplicates except a single copy\n",
    "olddf = olddf[~(olddf.duplicated(keep='last',\n",
    "                              subset=['APINumber','JobEndDate',\n",
    "                                      'JobStartDate','OperatorName',\n",
    "                                      'TotalBaseWaterVolume']))]\n",
    "print(f'Len old (no dupes): {len(olddf)},  Len new: {len(newdf)}')\n",
    "olddf['UploadKey'] = olddf.pKey\n",
    "print(olddf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83812d3-2254-40f4-aee9-c91f338f6cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 3\n",
    "mg = pd.merge(newdf, olddf[['UploadKey','APINumber','JobEndDate','JobStartDate','OperatorName','TotalBaseWaterVolume']],\n",
    "             on = ['APINumber','JobEndDate','JobStartDate','OperatorName','TotalBaseWaterVolume'],\n",
    "             how='outer',indicator=True, validate='m:1')\n",
    "mg._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0911da-aeae-4419-836f-457881f4af57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trim to just the inner join\n",
    "mg = mg[mg._merge=='both']\n",
    "upk = mg.UploadKey.unique().tolist()\n",
    "mg.drop('_merge',axis=1,inplace=True)\n",
    "# get old upload_dates\n",
    "old_upl = pd.read_parquet(old_upload_dates_fn)\n",
    "old_upl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae3729-a890-4724-acfc-4a2d1d5a3478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mg = pd.merge(mg,old_upl, on='UploadKey',how='outer',indicator=True,validate='m:1')\n",
    "mg._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a51d3-e353-42e8-a597-9263a50d959a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mg = mg[['DisclosureId','UploadKey','date_added','num_records','weekly_report']] \n",
    "mg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7808b7-73a5-426f-9f63-a36dd29b9be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to use as upload_dates.parquet in next build (transfer to orig_dir/curation_files)\n",
    "mg.to_parquet('./sandbox/upload_dates.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cbc9a-14c9-4454-a49b-f3ded8e9f07d",
   "metadata": {},
   "source": [
    "---\n",
    "# Preparing the dup_rec map for the disclosures before FFV4\n",
    "FF changed the format of \"empty\" Supplier, Purpose and TradeName fields which broke the way that I detected duplicated records.  While I wait (forever?)  for a solution from FracFocus to deal with such problems, I make, here, a file that can be used in later builds to still remove those records detected pre-FFV4.\n",
    "\n",
    "The file will have the DisclosureID and the necessary other fields needed to identify the already detected duplicates.\n",
    "\n",
    "Note that because of disclosure duplicates (which are filtered in the standard set) we delete the duplicate DisclosureId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753c112-1983-42cd-965e-a62d4bce447e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mg = pd.read_parquet('./sandbox/upload_dates.parquet')\n",
    "\n",
    "fn = r\"C:\\MyDocs\\integrated\\repos\\openFF_data_2023_11_25\\full_df.parquet\"\n",
    "df = pd.read_parquet(fn, columns=['UploadKey','CASNumber','IngredientName','PercentHFJob',\n",
    "                                  'PercentHighAdditive','MassIngredient','dup_rec'])\n",
    "df = df[df.dup_rec]\n",
    "out = pd.merge(df,mg[~mg.UploadKey.duplicated()][['UploadKey','DisclosureId']],on='UploadKey',how='left',validate='m:1')\n",
    "out.to_parquet('./sandbox/trans_dup_res.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a711e-48ce-4a9c-8ace-e4d074908031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn = r\"C:\\MyDocs\\integrated\\openFF\\build\\sandbox\\final\\full_df.parquet\"\n",
    "df = pd.read_parquet(fn)\n",
    "out.rename({'dup_rec':'old_dup_rec'},axis=1,inplace=True)\n",
    "out.drop('UploadKey',axis=1,inplace=True)\n",
    "mg = pd.merge(df,out[~out.duplicated()],on=['DisclosureId','CASNumber','IngredientName','PercentHFJob',\n",
    "                         'PercentHighAdditive','MassIngredient'],how='outer',validate='m:1',indicator=True)\n",
    "mg._merge.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc38612-5f05-4a71-9624-da72362955f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mg[mg.dup_rec & mg.old_dup_rec].FFVersion_x.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16858e-7df2-42f3-91c5-70c70b3abe4c",
   "metadata": {},
   "source": [
    "## Other changes to FracFocus data\n",
    "\n",
    "- Some changes to the CASNumber field: it appears that they are now filtering a little bit more (maybe dropping leading zeros?) so that means that some of the CASNumbers I resolved before are showing up again as unresolved.  \n",
    "- It appears that FF has started to set invalid or out of range lat and lon values to NaN.  This initially broke my reproject code.  I've set them to dummy values (0,0)\n",
    "- Water source table\n",
    "- Dropping many unsued or poorly used fields\n",
    "- The FracFocus README now includes `MassIngredient` suggesting that we can use it as a valid input to our reported masses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
